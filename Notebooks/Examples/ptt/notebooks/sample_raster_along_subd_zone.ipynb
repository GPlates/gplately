{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright (C) 2017 The University of Sydney, Australia\n",
    "\n",
    " This program is free software; you can redistribute it and/or modify it under\n",
    " the terms of the GNU General Public License, version 2, as published by\n",
    " the Free Software Foundation.\n",
    "    \n",
    " This program is distributed in the hope that it will be useful, but WITHOUT\n",
    " ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n",
    " FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n",
    " for more details.\n",
    "    \n",
    " You should have received a copy of the GNU General Public License along\n",
    " with this program; if not, write to Free Software Foundation, Inc.,\n",
    " 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
    " \n",
    " Authors: John Cannon and Simon Williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate subducting rate of paleo rasters at each age by sampling reconstructed rasters along subduction zones\n",
    "The notebook also outputs total subduction zone length.\n",
    "Note that this computation is not always accurate if the\n",
    "plate topology model is not clean in the sense that there are\n",
    "duplicated lines (instead of single lines, each shared by two adjoining plates) â€“ this is the case\n",
    "for the published AREPS plate motion model by Muller et al. (2016)\n",
    "and these bugs will be fixed in the future.\n",
    "Subduction zone lengths from models with such topological artefacts\n",
    "need to be computed using the workflow described here (which attempts to detect and remove duplicated lines):\n",
    "https://zenodo.org/record/154001#.WhooR7RdIkR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import os\n",
    "import pygplates\n",
    "import sys\n",
    "# Add directory containing the 'ptt' module (Plate Tectonic Tools) to the Python path.\n",
    "from ptt import subduction_convergence\n",
    "\n",
    "# Input rotation and topology files.\n",
    "rotation_filename = '../data/Global_EarthByte_230-0Ma_GK07_AREPS.rot'\n",
    "rotation_model = pygplates.RotationModel(rotation_filename)\n",
    "topology_filenames = [\n",
    "        '../data/Global_EarthByte_230-0Ma_GK07_AREPS_PlateBoundaries.gpmlz',\n",
    "        '../data/Global_EarthByte_230-0Ma_GK07_AREPS_Topology_BuildingBlocks.gpmlz']\n",
    "\n",
    "# Output file containing results at each reconstruction time.\n",
    "output_filename = '../data/subducting_raster.txt'\n",
    "\n",
    "# Base filename and extension of raster to sample.\n",
    "raster_filename_base = '../data/CO2'\n",
    "raster_filename_ext = 'nc'\n",
    "\n",
    "# Define the time range.\n",
    "# The reconstruction time range (topologies resolved to these times).\n",
    "# Also used to get paleo raster filenames based on 'raster_filename_base'.\n",
    "min_time = 0\n",
    "max_time = 1\n",
    "\n",
    "# Tessellate the subduction zones to 0.5 degrees.\n",
    "tessellation_threshold_radians = math.radians(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sample grids\n",
    "#\n",
    "# The method used here for sampling of masked (NaN) rasters is faster than using the 'raster_query' module.\n",
    "\n",
    "try:\n",
    "    from netCDF4 import Dataset as netcdf\n",
    "except ImportError:\n",
    "    from scipy.io import netcdf_file as netcdf\n",
    "    print('Warning: NetCDF4 grids not supported (\"netCDF4\" Python module not found). '\n",
    "          'Falling back to NetCDF3, rasters may fail to load.', file=sys.stderr)\n",
    "\n",
    "import scipy.interpolate as spi\n",
    "import numpy as np\n",
    "\n",
    "def sample_grid_using_scipy(x,y,grdfile):\n",
    "    \n",
    "    data=netcdf(grdfile,'r')\n",
    "    try:\n",
    "        lon = np.copy(data.variables['x'][:])\n",
    "        lat = np.copy(data.variables['y'][:])\n",
    "    except:\n",
    "        lon = np.copy(data.variables['lon'][:])\n",
    "        lat = np.copy(data.variables['lat'][:])\n",
    "    \n",
    "    Zg = data.variables['z'][:]\n",
    "    \n",
    "    test = fill_ndimage(Zg)\n",
    "    \n",
    "    lut=spi.RectBivariateSpline(lon,lat,test.T)\n",
    "    result = []\n",
    "    for xi,yi in zip(x,y):\n",
    "        result.append(lut(xi, yi)[0][0])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "def fill_ndimage(data,invalid=None):\n",
    "    \"\"\"Replace the value of invalid 'data' cells (indicated by 'invalid')\n",
    "    by the value of the nearest valid data cell\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array of any dimension\n",
    "    invalid: a binary array of same shape as 'data'. True cells set where data\n",
    "    value should be replaced.\n",
    "    If None (default), use: invalid = np.isnan(data)\n",
    "    Returns\n",
    "    -------\n",
    "    Return a filled array.\n",
    "    Credits\n",
    "    -------\n",
    "    http://stackoverflow.com/a/9262129\n",
    "    \"\"\"\n",
    "    if invalid is None: invalid = np.isnan(data)\n",
    "    ind = nd.distance_transform_edt(invalid, return_distances=False, return_indices=True)\n",
    "    return data[tuple(ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element of this list will be a tuple of values for a specific reconstruction time.\n",
    "output_data = []\n",
    "\n",
    "# Reconstruction times.\n",
    "reconstruction_times = range(min_time, max_time + 1)\n",
    "\n",
    "# Iterate over time steps\n",
    "for reconstruction_time in reconstruction_times:\n",
    "    \n",
    "    # Use existing subduction convergence script to generate sample points along subduction zones at 'time'.\n",
    "    subduction_convergence_data = subduction_convergence.subduction_convergence(\n",
    "                rotation_model,\n",
    "                topology_filenames,\n",
    "                tessellation_threshold_radians,\n",
    "                reconstruction_time)\n",
    "    \n",
    "    # Determine paleo raster filename.\n",
    "    raster_filename = '{0}_{1}.{2}'.format(raster_filename_base, reconstruction_time, raster_filename_ext)\n",
    "    \n",
    "    # Sample raster/grid at subduction points.\n",
    "    subduction_lons = [data[0] for data in subduction_convergence_data]\n",
    "    subduction_lats = [data[1] for data in subduction_convergence_data]\n",
    "    raster_values = sample_grid_using_scipy(subduction_lons, subduction_lats, raster_filename)\n",
    "    \n",
    "    #\n",
    "    # Do something with subducting raster values. Here we calculate average raster value.\n",
    "    #\n",
    "    total_subducting_length_metres = 0.0\n",
    "    total_subducting_raster_per_year = 0.0\n",
    "    total_raster_value_times_subduction_length = 0.0\n",
    "    total_raster_value_squared_times_subduction_length = 0.0\n",
    "    total_convergence_normal_velocity_times_subduction_length = 0.0\n",
    "    total_convergence_normal_velocity_squared_times_subduction_length = 0.0\n",
    "    total_raster_value_times_convergence_normal_velocity_times_subduction_length = 0.0\n",
    "    total_raster_value_squared_times_convergence_normal_velocity_squared_times_subduction_length = 0.0\n",
    "    for subduction_point_index, raster_value in enumerate(raster_values):\n",
    "\n",
    "        # If couldn't find a non-NaN raster value within 20 degrees then skip to next subduction point.\n",
    "        if math.isnan(raster_value):\n",
    "            continue\n",
    "\n",
    "        # We've got some other subduction data besides lon/lat locations.\n",
    "        subduction_convergence_item = subduction_convergence_data[subduction_point_index]\n",
    "\n",
    "        #lon = subduction_convergence_item[0]\n",
    "        #lat = subduction_convergence_item[1]\n",
    "        convergence_velocity_magnitude_cm_per_yr = subduction_convergence_item[2]\n",
    "        convergence_obliquity_degrees = subduction_convergence_item[3]\n",
    "        #absolute_velocity_magnitude = subduction_convergence_item[4]\n",
    "        #absolute_obliquity_degrees = subduction_convergence_item[5]\n",
    "        subducting_length_degrees = subduction_convergence_item[6]\n",
    "        #subducting_arc_normal_azimuth = subduction_convergence_item[7]\n",
    "        #subducting_plate_id = subduction_convergence_item[8]\n",
    "        #subduction_zone_plate_id = subduction_convergence_item[9]\n",
    "\n",
    "        subducting_length_metres = (\n",
    "            math.radians(subducting_length_degrees) * 1e3 * pygplates.Earth.mean_radius_in_kms)\n",
    "        \n",
    "        convergence_normal_velocity_metres_per_year = (\n",
    "            # 1e-2 converts cm/y to m/y...\n",
    "            1e-2 * math.fabs(convergence_velocity_magnitude_cm_per_yr) *\n",
    "            # Negative convergence handled by cos(obliquity_angle)...\n",
    "            math.cos(math.radians(convergence_obliquity_degrees)))\n",
    "\n",
    "        total_subducting_length_metres += subducting_length_metres\n",
    "        \n",
    "        #\n",
    "        # If raster is a thickness in metres then 'total_subducting_raster_per_year' has units:\n",
    "        #\n",
    "        #   m^3/y = m * m * m/y\n",
    "        #\n",
    "        # ...which is a total volume rate.\n",
    "        #\n",
    "        # Otherwise if the raster is a quantity per raster pixel such as 'kg/m^2'\n",
    "        # then 'total_subducting_raster_per_year' has units:\n",
    "        #\n",
    "        #   kg/y = kg/m^2 * m * m/y\n",
    "        #\n",
    "        # ...which is a total mass rate.\n",
    "        #\n",
    "        total_subducting_raster_per_year += (\n",
    "                raster_value * subducting_length_metres * convergence_normal_velocity_metres_per_year)\n",
    "\n",
    "        total_raster_value_times_subduction_length += raster_value * subducting_length_metres\n",
    "        total_raster_value_squared_times_subduction_length += raster_value * raster_value * subducting_length_metres\n",
    "        \n",
    "        total_convergence_normal_velocity_times_subduction_length += (\n",
    "                convergence_normal_velocity_metres_per_year * subducting_length_metres)\n",
    "        total_convergence_normal_velocity_squared_times_subduction_length += (\n",
    "                convergence_normal_velocity_metres_per_year * convergence_normal_velocity_metres_per_year *\n",
    "                subducting_length_metres)\n",
    "        \n",
    "        total_raster_value_times_convergence_normal_velocity_times_subduction_length += (\n",
    "                raster_value * convergence_normal_velocity_metres_per_year * subducting_length_metres)\n",
    "        total_raster_value_squared_times_convergence_normal_velocity_squared_times_subduction_length += (\n",
    "                raster_value * raster_value *\n",
    "                convergence_normal_velocity_metres_per_year * convergence_normal_velocity_metres_per_year *\n",
    "                subducting_length_metres)\n",
    "\n",
    "    #\n",
    "    # Statistics.\n",
    "    #\n",
    "    # mean = M = sum(Ci * Xi) / sum(Ci)\n",
    "    # std_dev  = sqrt[sum(Ci * (Xi - M)^2) / sum(Ci)]\n",
    "    #          = sqrt[(sum(Ci * Xi^2) - 2 * M * sum(Ci * Xi) + M^2 * sum(Ci)) / sum(Ci)]\n",
    "    #          = sqrt[(sum(Ci * Xi^2) - 2 * M * M * sum(Ci) + M^2 * sum(Ci)) / sum(Ci)]\n",
    "    #          = sqrt[(sum(Ci * Xi^2) - M^2 * sum(Ci)) / sum(Ci)]\n",
    "    #          = sqrt[(sum(Ci * Xi^2) / sum(Ci) - M^2]\n",
    "    #\n",
    "    # ...where N is total number of sample points, Xi are sample and Ci are weights.\n",
    "    #\n",
    "    \n",
    "    mean_subducting_raster_value = total_raster_value_times_subduction_length / total_subducting_length_metres\n",
    "    variance_subducting_raster_value = (total_raster_value_squared_times_subduction_length / total_subducting_length_metres -\n",
    "                                       mean_subducting_raster_value * mean_subducting_raster_value)\n",
    "    std_dev_subducting_raster_value = (math.sqrt(variance_subducting_raster_value)\n",
    "                                      if variance_subducting_raster_value > 0.0 else 0.0)\n",
    "    \n",
    "    mean_convergence_normal_velocity_metres_per_year = (\n",
    "            total_convergence_normal_velocity_times_subduction_length / total_subducting_length_metres)\n",
    "    variance_convergence_normal_velocity_metres_per_year = (\n",
    "            total_convergence_normal_velocity_squared_times_subduction_length / total_subducting_length_metres -\n",
    "            mean_convergence_normal_velocity_metres_per_year * mean_convergence_normal_velocity_metres_per_year)\n",
    "    std_dev_convergence_normal_velocity_metres_per_year = (math.sqrt(variance_convergence_normal_velocity_metres_per_year)\n",
    "                                      if variance_convergence_normal_velocity_metres_per_year > 0.0 else 0.0)\n",
    "    \n",
    "    mean_raster_value_times_convergence_normal_velocity = (\n",
    "            total_raster_value_times_convergence_normal_velocity_times_subduction_length /\n",
    "            total_subducting_length_metres)\n",
    "    variance_raster_value_times_convergence_normal_velocity = (\n",
    "            total_raster_value_squared_times_convergence_normal_velocity_squared_times_subduction_length /\n",
    "                total_subducting_length_metres -\n",
    "            mean_raster_value_times_convergence_normal_velocity * mean_raster_value_times_convergence_normal_velocity)\n",
    "    std_dev_raster_value_times_convergence_normal_velocity = (\n",
    "            math.sqrt(variance_raster_value_times_convergence_normal_velocity)\n",
    "            if variance_raster_value_times_convergence_normal_velocity > 0.0 else 0.0)\n",
    "    \n",
    "    # Add a tuple of data values.\n",
    "    # Can be any number of elements (they'll get written as columns to the output text file in the same order).\n",
    "    output_data.append((\n",
    "            reconstruction_time,\n",
    "            mean_subducting_raster_value,\n",
    "            std_dev_subducting_raster_value,\n",
    "            mean_convergence_normal_velocity_metres_per_year,\n",
    "            std_dev_convergence_normal_velocity_metres_per_year,\n",
    "            mean_raster_value_times_convergence_normal_velocity,\n",
    "            std_dev_raster_value_times_convergence_normal_velocity,\n",
    "            total_subducting_length_metres,\n",
    "            total_subducting_raster_per_year))\n",
    "\n",
    "    print ('reconstruction_time: ', reconstruction_time)\n",
    "    print ('mean_subducting_raster_value: ', mean_subducting_raster_value)\n",
    "    print ('std_dev_subducting_raster_value: ', std_dev_subducting_raster_value)\n",
    "    print ('mean_convergence_normal_velocity_metres_per_year: ', mean_convergence_normal_velocity_metres_per_year)\n",
    "    print ('std_dev_convergence_normal_velocity_metres_per_year: ', std_dev_convergence_normal_velocity_metres_per_year)\n",
    "    print ('mean_raster_value_times_convergence_normal_velocity: ', mean_raster_value_times_convergence_normal_velocity)\n",
    "    print ('std_dev_raster_value_times_convergence_normal_velocity: ', std_dev_raster_value_times_convergence_normal_velocity)\n",
    "    print ('total_subducting_length_metres: ', total_subducting_length_metres)\n",
    "    print ('total_subducting_raster_per_year: ', total_subducting_raster_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output data to a text file.\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    for output_line in output_data:\n",
    "        # Each 'output_line' is a tuple of values for a specific reconstruction time.\n",
    "        # Write the elements of the tuple separated by spaces (and ending with a newline).\n",
    "        output_file.write(' '.join(str(item) for item in output_line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
